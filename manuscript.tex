\documentclass[11pt]{article}


\usepackage{geometry}

\usepackage{amsmath,amssymb}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\card}{card}
\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\Det}{Det}
\DeclareMathOperator*{\trace}{tr}

\usepackage{bm}
\newcommand{\mat}[1]{\mathbf{#1}}

\title{Fast gradient-based optimization of resistance surfaces}
\author{Nathaniel S. Pope\footnote{Department of Entomology, the Pennsylvania State University, University Park, PA 16802} \footnote{nsp5229@psu.edu}}
\date{}

\begin{document}

\maketitle

Key point for introduction: this is a flexible framework for resistance surface optimization,
that is fast because (A) it only requires a single Cholesky decomposition; (B) it uses analytic
gradients with little additional computational cost.

\section{Isolation by resistance as a non-linear model}

The fundamental idea behind isolation by resistance is that a landscape may be
discretized into a fully-connected graph (e.g. a lattice), and the movement of
organisms modelled as a random walk on this graph. One of the simplest models
choices of random walk is a symmetric, continuous-time Markov process: random
walkers at the $i$th vertex of the graph transition to an adjacent vertex $j$
at rate $\lambda_{ij}$ (where $\lambda_{ij} = \lambda_{ji}$). Competing
hypotheses about how landscape heterogeneity impacts the movement of organisms
may be directly encoded by particular choices of transition rates
(``conductances'') among graph vertices: thus this framework has had great
utility in landscape ecology since its introduction by McRae (2005).

A measure of connectivity that arises naturally from this framework is the
commute distance (aka ``resistance distance'') between two locations: this is
the expected time taken by a random walker to move from one point to another
point B and back again. The infintesimal generator of the Markov process is the
graph Laplacian $\mat Q$ with entries
\[
  \mat Q_{ij} = \begin{cases} 
    -\lambda_{ij} & \mathrm{if~} i \neq j, j \in \mathcal{N}_i \\ 
    \sum_k \lambda_{ik} & \mathrm{if~} i = j \\
    0 & \mathrm{otherwise}
  \end{cases}
\]
where $\mathcal{N}_i$ is the set of vertices that are neighbours to vertex $i$.
$\mat Q$ is thus a very sparse, symmetric, positive semidefinite matrix of rank
$N-1$, where $N$ is the number of vertices in the graph. Let $\mat Z$ be an $N$
by $M$ indicator matrix mapping a set of $M$ spatial locations to vertices,
so that $\mat Z_{ik} = 1$ if the $k$th spatial location is at vertex
$i$, and $0$ otherwise. The matrix of resistance distances between the spatial
locations encoded by $\mat Z$ is
\begin{equation}
  \label{resistance}
  \mat R = \diag \{ \mat Z^T \mat Q^+ \mat Z \} \mat 1^T + 
           \mat 1 \diag \{\mat Z^T \mat Q^+ \mat Z \}^T - 
           2 \mat Z^T \mat Q^+ \mat Z
\end{equation}
where $\mat Q^+$ is the generalized inverse of $\mat Q$. Ideally, resistance
distances would be compared to observed migration rates, and a strong
correspondence between the two would be taken as evidence for the underlying
hypothesis of landscape conductance to movement. In practice, migration is
difficult to measure directly, and genetic divergence among spatially-indexed
individuals is used as a proxy.

%NB: I think I need to state that x can be dummy-coded categorical variables
For the remainder of this paper, I model the conductance of an edge connecting
vertices $i$ and $j$ as separable and additive, so that $\lambda_{ij} = \mat
C_i + \mat C_j$. For the $i$th vertex, I model the contribution to conductance
as a function of a vector of covariates $\mat x_i$, so that $\mat C_i = f(\mat
x_i; \bm \theta)$, where $\bm \theta$ is a vector of unknown parameters and
$\mat C_i > 0$.  For example, a log-linear model for vertex conductance is
$f(\mat x_i; \bm \theta) = \exp\{\mat x_i^T \bm \theta\}$ (an intercept is
omitted as it simply rescales $\mat R$). I refer to $f$ as the ``conductance
model'', and to the vector $\mat C(\bm \theta) = [\mat C_1, \dots, \mat C_N]$
as the ``conductance surface'' generated by $\bm \theta$.

Let $\mat Y$ be either the genotypes of individuals collected across the
locations encoded by $\mat Z$, or a function of these genotypes such as a
genetic dissimilarity metric. Let $g(\mat Y; \mat R)$ be the probability of
$\mat Y$ given resistance distances $\mat R$. For example, $g$ might be the
likelihood of a linear regression between genetic relatedness and resistance
distance (I discuss possible choices of $g$ in detail below). I refer to $g$ as
the ``measurement model''. The log-likelihood of $\bm \theta$ given the genetic
data and spatial covariates is $\mathcal{L}(\bm \theta) = \log g(\mat Y; \mat
R(\mat Q(\mat C(\bm \theta))))$.  The maximum likelihood estimate $\hat{\bm
  \theta} = \argmax \mathcal{L}(\bm \theta)$ provides an estimate of the
conductance surface $\mat C(\hat{\bm \theta})$ for a given set of spatial
covariates. 

Model selection -- that is, choosing among different sets of spatial covariates
or different functional forms for $f$ -- can proceed via standard practices for
maximum likelihood inference, such as likelihood ratio tests, cross-validation,
or information criteria, using the number of free values in $\theta$ as the
degrees of freedom. For example, the log-likelihood under the null model of
isolation by distance is $\mathcal{L}_{\mathrm{IBD}} = \log g(\mat Y; \mat
R(\mat Q(\mat C(\mat 0))))$ where $\mat C(\mat 0) = [1, \dots, 1]$. For the
log-linear model $\mat C_i = \exp \{ \mat x_i^T \bm \theta \}$ (and for many other
reasonable choices of $f$), the null model is on the interior of the parameter
space (e.g. the point where $\bm \theta = 0$), so $2 \mathcal{L} (\hat{\bm \theta}) -
2 \mathcal{L}_\mathrm{IBD}$ is asymptotically distributed as 
$\chi^2(\| \bm{\hat \theta} \|_0)$.

Many prior studies (REF) have ... This is not ideal: ... However, the framework
described above is not novel: several prior works have also considered
optimization in frameworks that are implicitly or explicitly similar to the one
described above. The challenge -- and the novel contribution of this study --
is to do so in a computationally efficient manner.

%that allows isolation by resistance models for moderate-to-large landscapes to
%be fit on the order of minutes rather than days.

\section{Efficient gradient calculation}

Provided that $g(\mat Y; \mat R)$ and $f(\bm \theta)$ are differentiable in
$\mat R$ and $\bm \theta$ respectively, first-order derivative information can
greatly accelarate optimization of $\mathcal{L}$,
especially when $\bm \theta$ has many elements. 
However, numeric approximation by Richardson extrapolation or finite differencing
is costly, because the large linear system in Equation \ref{resistance} must be
solved for every perturbation of the parameter vector.
%Efficient gradient calculation also enables fast computation of the observed Fisher information, providing asymptotic standard errors for $\hat{\bm \theta}$ \cite{efronFisherInfo}. 
Here, I use reverse algorithmic differentiation (repeated application of the chain
rule in reverse) to give an exact algorithm for the gradient
$\dot{\bm \theta} = [\partial \mathcal{L}/\partial \bm \theta_1, \dots, \partial \mathcal{L}/\partial \bm \theta_K]$ 
that requires virtually no computational cost beyond that already used to calculate
$\mathcal{L}$.

%TODO: there are some issues with incorrect transposition. Correct based on R code
The graph Laplacian $\mat Q$ has a single zero eigenvalue, and its nullspace is
spanned by the vector $\mat v = N^{-1/2} \mat 1$ \cite{missing}. Let the rank $N-1$
diagonalization of $\mat Q = \mat P \mat D \mat P^T$ (e.g. the columns of $\mat
P$ are the $N-1$ eigenvectors spanning the column space). As the column space
and nullspace are orthogonal, $\mat P \mat P^T + \mat v \mat v^T = \mat I$. 
Let $\mat I_{-N}$ be the $N$-dimensional identity matrix with column $N$ removed. Then, 
\[ 
  \begin{aligned} 
    \mat Q^+ = \mat P \mat D^{-1} \mat P^T 
           & = \mat P \mat P^T \mat I_{-N}^T (\mat P^T \mat I_{-N}^T)^{-1} \mat D^{-1} (\mat I_{-N} \mat P)^{-1} \mat I_{-N} \mat P \mat P^T \\ 
           & = (\mat I_{-N}^T - \mat v \mat v^T \mat I_{-N}^T) (\mat P^T \mat I_{-N}^T)^{-1} \mat D^{-1} (\mat I_{-N} \mat P)^{-1} (\mat I_{-N} - \mat v \mat v^T \mat I_{-N}) \\ 
           & = (\mat I_{-N}^T - \mat v \mat v^T \mat I_{-N}^T) (\mat I_{-N} \mat Q \mat I_{-N}^T)^{-1} (\mat I_{-N} - \mat v \mat v^T \mat I_{-N}).
  \end{aligned} 
\] 
From a computational perspective, this identity is very useful, because the
problem in Equation \ref{resistance} reduces to solving 
$\mat Q_{-N} \mat G = \mat Z_{-N}$ for $\mat G$, where 
$\mat Q_{-N} = \mat I_{-N} \mat Q \mat I_{-N}^T$ and 
$\mat Z_{-N} = (\mat I_{-N} - \mat v \mat v^T \mat I_{-N}) \mat Z$, 
so that
\[
  \mat R = \diag \{ \mat Z_{-N}^T \mat G \} \mat 1^T + \mat 1 \diag \{\mat Z_{-N}^T \mat G \}^T - 2 \mat Z_{-N}^T \mat G.
\]
This requires a single Cholesky decomposition of the very sparse, full-rank
matrix $\mat Q_{-N}$ (algorithm \ref{RDalgo}A). Other algorithms in use (e.g. \cite{missing})
require the solution of $M$ independent linear systems (equivalently $M$
Cholesky decompositions). The identity derived above is closely related to the
method described by \cite{missing}.

%NB: be sure to explicitely define symmetrized \dot R for examples
Let $\dot{\mat R}$ be the symmetric matrix of partial derivatives of $\log g(\mat Y; \mat R)$
with respect to resistance distances $\mat R$. This is typically cheap to
compute (specific examples are given below). Using the chain rule in reverse, 
the gradient with respect to the $i,j$th element of $\mat Q_{-N}$ is:
\begin{equation}
  \label{qdiff}
  (\dot{\mat Q}_{-N})_{ij} = \begin{cases} 
    \mat G_i ( \mat I \circ \dot{\mat R} \mat 1 \mat 1^T - \dot{\mat R} ) \mat G_i^T & \mathrm{if~} i = j \\
    -\mat G_i ( \mat I \circ \dot{\mat R} \mat 1 \mat 1^T - \dot{\mat R} ) \mat G_j^T & \mathrm{if~} i \neq j, j \in \mathcal{N}_i \\
    0 & \mathrm{otherwise,}
  \end{cases}
\end{equation}
where $\mat G_i$ is the $i$th row of $\mat G$ and $\circ$ is the
element-wise product. The gradient with respect to the elements in the
conductance surface is
\[
  \dot{\mat C}_i = \begin{cases}
    n_i [\dot{\mat Q}_{-N}]_{ii} + \sum_{j \in \mathcal{N}_i \setminus N } [\dot{\mat Q}_{-N}]_{jj} - 2 [\dot{\mat Q}_{-N}]_{ij} & \mathrm{if~} i \neq N \\
    \sum_{j \in \mathcal{N}_N} [\dot{\mat Q}_{-N}]_{jj} & \mathrm{if~} i = N,
  \end{cases}
\]
where $n_i = \card \{ \mathcal{N}_i \}$ is the number of neighbours of vertex $i$. This scheme
is very computationally efficient as it involves iterating over only the
non-zero elements of $\mat Q$ and utilizes the previously computed 
$\mat G$ (algorithm \ref{RDalgo}B). Finally, 
$\dot{\bm \theta}_k = \sum_i \dot{\mat C}_i (\partial{\mat C_i} / \partial{\bm \theta_k})$. 
For example, $\partial{\mat C_i} / \partial{\bm \theta_k} = \mat x_{ki} \mat C_i$
for the log-linear model $\mat C_i = \exp\{ \mat x_i^T \bm \theta \}$. Importantly,
this calculation is exact and the computational cost is essentially independent of the
dimension of $\bm \theta$, unlike gradient approximation by finite differencing.

\section{Measurement models}

Here I derive the gradient $\dot{\mat R}$ required in the previous section for
four reasonable choices of $g(\mat Y; \mat R)$.  Let $\mat S$ be a genetic
dissimilarity matrix calculated from genotypes $\mat Y$.  Let $\mat s, \mat r$ be
the lower triangular vectorizations of $\mat S, \mat R$ (e.g. the ${M \choose 2}$ values of $\mat R_{ij}$ 
where $i < j$). For clarity, I retain matrix indexing despite vectorization: e.g. $\mat r_{ij} =
\mat R_{ij}$.

% NB. Ignoring scaling of \mat r
\emph{Naive regression}. 
The model is $\mat s =  \mat X \bm \beta + \bm \epsilon$, where
$\mat X = [\mat 1, \mat r]$ and $\bm \epsilon$ are \emph{i.i.d.} Gaussian errors with standard deviation
$\sigma$. Given $\mat r$, the maximum likelihood estimates for the nuisance
parameters are $\hat{\bm \beta} = (\mat X^T \mat X)^{-1} \mat X^T \mat s$ and 
$\hat{\sigma}^2 = {M \choose 2}^{-1} \hat{\mat e}^T \hat{\mat e}$ where 
$\hat{\mat e} = \mat s - \mat X \hat{\bm \beta}$.  Thus
$\log g_{\mathrm{reg}}(\mat Y; \mat R) = -2^{-1} \hat{\sigma}^{-2} \hat{\mat e}^T \hat{\mat e} - {M \choose 2} \log \hat{\sigma}$, and
\[
  [\dot{\mat R}_\mathrm{reg}]_{ij} = [\dot{\mat R}_\mathrm{reg}]_{ji} = 2 \hat{\sigma}^{-2} \hat{\bm \beta}_2 \hat{\mat e}_{ij}.
\]

\emph{Maximum likelihood population effects}.
Naive regression between $\mat s$ and $\mat r$ assumes that the errors are
independent, which is unlikely to hold as $\mat s$ contains pairwise
measurements. The maximum likelihood population effects (MLPE) model of
\cite{clarkeMLPE} attempts to model the dependence between pairwise
observations by introducing \emph{i.i.d.} Gaussian effects
$\bm \gamma$, one for each spatial location:
$\mat s_{ij} = [\mat X \bm \beta]_{ij} + \bm \gamma_i + \bm \gamma_j + \bm
\epsilon_{ij}$.  Let $\mat U$ be an ${M \choose 2}$-by-$M$ indicator matrix
mapping spatial locations to pairwise observations, so that $\mat U_{ij,k} = 1$
if $k \in \{i, j\}$ and $0$ otherwise.  After integrating over $\bm \gamma$, the
correlation of $\mat s$ becomes $\bm \Sigma = (1 - 2\rho) \mat I + \rho \mat U \mat U^T$ and 
the maximum likelihood estimates are 
$\hat{\bm \beta} = (\mat X^T \bm \Sigma^{-1} \mat X)^{-1} \mat X^T \bm \Sigma^{-1} \mat s$ and
$\hat{\sigma} = {M \choose 2}^{-1} \hat{\mat e}^T \bm \Sigma^{-1} \hat{\mat e}$ where 
$\hat{\mat e} = \mat s - \mat X \hat{\bm \beta}$. 
Thus $\log g_{\mathrm{mlpe}}(\mat Y; \mat R, \rho) = -2^{-1} \hat{\sigma}^{-2} \hat{\mat e}^T \bm \Sigma^{-1} \hat{\mat e} - 2^{-1} {M \choose 2} \log \det (\sigma^2 \bm \Sigma)$, and
\[
  [\dot{\mat R}_\mathrm{mlpe}]_{ij} = [\dot{\mat R}_\mathrm{mlpe}]_{ji} = 2 \hat{\sigma}^{-2} \hat{\bm \beta}_2 [\bm \Sigma^{-1} \hat{\mat e}]_{ij}.
\]
The nuisance parameter $\rho$ has no closed form optimum and must be optimized along with $\bm
\theta$; note that $\partial \mathcal{L} / \partial \rho = \trace\{ (\mat I - \sigma^{-2} \bm \Sigma^{-1} \hat{\mat e} \hat{\mat e}^T) \bm \Sigma^{-1} (\mat I - 2^{-1} \mat U^T \mat U) \}$.   
Any matrix-vector product involving $\bm \Sigma^{-1}$ may be efficiently computed via the
Woodbury lemma \cite{woodbury}.

%NB: Cite Holbrook 2018 Differentiating the pseudo determinant. Linear Algebra and its Applications 548 (1) : 293-304
\emph{Generalized Wishart.} \cite{peterson2019} suggest using $\mat E = \mat Z^T \mat Q^+ \mat Z$ itself
as a covariance structure for the normalized genotypes. For simplicity, assume the
genotypes are biallelic across $L$ loci -- although what follows can be
extended to multiallelic markers -- so that $\mat Y$ and $\mat N$ are $L$-by-$M$
matrices of derived allele counts and number of sampled haplotypes,
respectively. For locus $l$, let the global allele frequencies $\mat F_l = (\sum_i \mat
N_{li})^{-1} \sum_i \mat Y_{li} $ and normalized genotypes $\tilde{\mat Y}_{li}
= (\mat N_{li} \mat F_l - \mat N_{li} \mat F_l^2)^{-1/2} (\mat Y_{li} - \mat
N_{li} \mat F_l)$.  Let the genetic dissimilarity matrix $\mat S = \mat 1 \diag
\{ \tilde{\mat Y}^T \tilde{\mat Y} \}^T + \diag \{ \tilde{\mat Y}^T \tilde{\mat
  Y} \} \mat 1^T - 2 \tilde{\mat Y}^T \tilde{\mat Y}$. If the columns of
$\tilde{\mat Y}^T$ are \emph{i.i.d.} multivariate Gaussian vectors with
covariance $\bm \Sigma = \tau^2 \mat E + \sigma^2 \mat I$, then $\mat S$
follows a generalized Wishart distribution \cite{mccullaugh}, and $\log
g_{\mathrm{wis}} (\mat Y; \mat E, \sigma^2) = 4^{-1} L \trace \{ \bm
\Sigma^{-1} \mat W \mat S \} + 2^{-1} L \log \Det \{ \mat \Sigma^{-1} \mat W
\},$ where $\Det$ is the pseudo-determinant (the product of non-zero
eigenvalues) and $\mat W = \mat I - \mat 1 (\mat 1^T \mat \Sigma^{-1} \mat 1)^{-1}
\mat 1^T \bm \Sigma^{-1}$ is a projection onto the column space of $\bm \Sigma^{-1}$.  Because $\mat E$ is used in place of $\mat R$, for the
calculation of $\dot{\bm \theta}$ Equation \ref{qdiff} should be replaced by
\begin{equation}
  \label{rdiff2}
  (\dot{\mat Q}_{-N})_{ij} = \begin{cases} 
    \mat G_i \dot{\mat E} \mat G_i^T & \mathrm{if~} i = j \\
    -\mat G_i \dot{\mat E} \mat G_j^T & \mathrm{if~} i \neq j, j \in \mathcal{N}_i \\
    0 & \mathrm{otherwise.}
  \end{cases}
\end{equation}
Using a result from \cite{holbrook2018} for differentiation of the pseudo-determinant,
\[
  %\dot{\mat E}_{\mathrm{wis}} = -2^{-1} L \tau^2 \bm \Sigma^{-1} \mat W  (\bm \Sigma^{-1} \mat W)^+ \mat W^T \bm \Sigma^{-1} - 4^{-1} L \tau^2 \bm \Sigma^{-1} \mat W \mat S \mat W^T \bm \Sigma^{-1}
  \dot{\mat E}_{\mathrm{wis}} = - L \tau^2 \bm \Sigma^{-1} \mat W  (2^{-1}(\bm \Sigma^{-1} \mat W)^+ + 4^{-1} \mat S) \mat W^T \bm \Sigma^{-1}
\]
The nuisance parameters $\sigma^2$ and $\tau^2$ must be optimized in tandem with $\bm \theta$; note that
$\partial \mathcal{L} / \partial \tau^2 = \tau^{-2} \trace \{ \mat E \dot{\mat E}_{\mathrm{wis}} \}$ and
$\partial \mathcal{L} / \partial \sigma^2 = \tau^{-2} \trace \{ \dot{\mat
  E}_{\mathrm{wis}} \}$. In practice, dissimilarity measures such as
$\mathrm{F}_{\mathrm{st}}$ could be substituted for the distance matrix $\mat S$ defined above,
although the rationale behind the derivation would be lost.

\emph{Binomial mixed model.} In some cases, the normalized genotypes
$\tilde{\mat Y}$ may not be well approximated by a Gaussian, for example when
there are only a few sampled haplotypes per spatial location (e.g. a single
diploid individual). It may be more appropriate to model the raw genotypes
$\mat Y$ as binomial. Let $\mat z_{il} = \alpha + \beta_l + \bm \epsilon_{il}$ be a latent variable, and let $\mat p_{il} = (1 + e^{-\mat z_{il}})^{-1}$ 
be the probability that a randomly sampled haplotype from location $i$ carries the derived allele at locus $l$. The parameter
$\alpha$ is the average allele frequency across loci, the per-locus effects $\beta_l$ are \emph{i.i.d.} Gaussian with standard deviation $\omega$,
and the errors $\bm \epsilon_l$ are Gaussian vectors with covariance $\tau^2 \mat E + \sigma^2 \mat I$. 
Thus $\mat z_l$ are Gaussian vectors with covariance 
$\mat \Sigma = \tau^2 \mat E + \omega^2 \mat 1 \mat 1^T + \sigma^2 \mat I$ and mean $\bm \mu = \alpha \mat 1$.
The likelihood is intractable, but can be reasonably approximated by Laplace's
method \cite{laplaceApprox}. Fisher scoring is used to find the maximum $\hat{\mat z}_l$ of $h(\mat z_l) = -2^{-1} (\mat z_l - \bm \mu)^T
\mat \Sigma^{-1} (\mat z_l - \bm \mu) + \sum_i \mat Y_{il} \log \mat p_{il} + (\mat N_{il}
- \mat Y_{il}) \log (1 - \mat p_{il}) $ (Appendix
\ref{glmmAppendix}).
Then, 
$\log g_{\mathrm{bin}}(\mat Y; \mat E, \alpha, \sigma^2, \tau^2, \omega^2) = 2^{-1} (L \log \det \mat \Sigma^{-1} - \sum_l h(\hat{\mat z}_l) - \log \det \mat H_{\hat{\mat z}_l})$ where
$[\mat H_{\hat{\mat z}_l}]_{ij} = [\mat \Sigma^{-1}]_{ij} - \mathbb{I}[i = j] \mat N_{il} \hat{\mat p}_{il} (1 - \hat{\mat p}_{il})$.
The adjoint method (Appendix \ref{glmmAppendix}) can be used to derive the gradient, 
\[
  \dot{\mat E}_{\mathrm{bin}} = ???,
\]
where ...
The nuisance parameters have gradients $\partial \mathcal{L}/\partial \alpha = ???, \partial \mathcal{L}/\partial \sigma^2 = ???, \partial \mathcal{L}/\partial \tau^2 = ..., \partial \mathcal{L}/\partial \omega^2 = ???$.

Both of the regression-based measurement models ($g_{\mathrm{reg}}$ and
$g_{\mathrm{mlpe}}$) allow an unconstrained linear relationship between $\mat r$ and
$\mat s$. Thus, for these choices of $g$, local optima of $\mathcal{L}(\bm
\theta)$ may exist where genetic distance \emph{decreases} with resistance
distance (e.g. $\hat{\bm \beta}_2 < 0$).  As this is implausible on biological
grounds, the model can be modified to incorporate the constraint $\bm \beta_2 >
0$. For the log-linear conductance $\mat C_i = \exp\{ x_i^T \bm \theta \}$,
this is accomplished by absorbing $\bm \beta_2$ into $\bm \theta$ (Appendix
\ref{reparamApp}).

\section{Hessian and leverage}
For both optimization and calculation of asymptotic standard errors, it is useful to have an efficient calculation of the Hessian.

Consider the second partial derivatives of the likelihood with respect to elements of $\mat Q_{-N}$. In other words:
\[
  \partial[ \partial \mathcal{L} / [\mat Q_{-N}]_{ij} ]/ \partial [\mat Q_{-N}]_{kl} = \partial [\dot{\mat Q}_{-N}]_{ij} / \partial [\mat Q_{-N}]_{kl}
\]
summing over $i,j$ and using standard matrix calculus,
\[
  \sum_{i,j} 
  \partial [\dot{\mat Q}_{-N}]_{ij} / \partial [\mat Q_{-N}]_{kl}
  =
  -[\mat Q^{-1}_{-N} \mat 1 \mat 1^T \mat Q^{-1}_{-N} \mat Z \dot{\mat E} \mat Z^T \mat Q^{-1}]_{kl}
\]
That's the second derivative of the likelihood with regard to elements in $Q_{-N}$.

%Basically we have (ignoring dot R/E for now)
% f(Q(theta)) * h(theta)
%that means that
% f'(theta) = f'(Q) Q'(theta)
%
%this is what I'm working on above.

%now think about it for dot E.
%dot E is a matrix-valued function of Q_ij, more specifically a matrix valued function of E(Q_ij).
%take a particular element of E, that is E_kl
%calculate the gradient of dot E wrt E_kl, we have then dE_kl




Have $\mathcal{L}(\hat{\theta}(x_i))$ where $x_i$ are the spatial covariates, and $\hat{\theta}(x_i)$ emphasizes that the MLE is a function of the spatial covariates.
We have $dL/dtheta \equiv 0$. Thus $d(dL/dtheta)/dx = 0$. And $d(dL/dtheta)/dx = d2L/dtheta2 dtheta/dx + dL/dx dtheta$. We know $dL/dx$ from above.

Finally, for the three methods with differentiable responses, can calculate leverage with regard to a measure of genetic distance.


\section{Implementation and benchmarking}
Numerical verification for the gradients derived in this paper is provided as a supplementary R script.

I recorded the time taken to compute likelihood and gradient versus likelihood alone, and the number of iterations
needed to find $\hat{\bm \theta}$ using gradient-free (BOBQYA) and gradient-based (BFGS) optimization algorithms, across 
varying numbers of sampled spatial locations and spatial covariates. As a point of comparison for my implementation,
I also used Circuitscape [version information, settings].

Simulate data from coalescent to test asymptotic bias, because measurement models used above are all drastic simplifications of actual geneological process.

\section{Example: gene flow in \emph{M. ???}}




\end{document}


